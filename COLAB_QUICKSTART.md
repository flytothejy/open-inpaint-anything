# ğŸš€ Google Colab ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ë²•

### 1. Google Colab ì¤€ë¹„
1. [Google Colab](https://colab.research.google.com) ì ‘ì†
2. ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±
3. **ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ â†’ GPU** ì„ íƒ

### 2. ìë™ ì„¤ì • ì‹¤í–‰ (í•œ ë²ˆì— ì™„ë£Œ)
ìƒˆ ì…€ì— ì•„ë˜ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”:

```python
# Colabìš© Inpaint Anything ìë™ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
!wget -q https://raw.githubusercontent.com/hllj/Inpaint-Anything/api-implement/colab_setup.py
!python colab_setup.py
```

### 3. ìˆ˜ë™ ì„¤ì • (ìœ„ ìë™ ì„¤ì •ì´ ì‹¤íŒ¨í•  ê²½ìš°)

#### 3.1 GPU í™•ì¸
```python
!nvidia-smi
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
```

#### 3.2 í”„ë¡œì íŠ¸ ì„¤ì •
```python
# í”„ë¡œì íŠ¸ í´ë¡ 
!git clone https://github.com/hllj/Inpaint-Anything.git
%cd Inpaint-Anything

# ì˜ì¡´ì„± ì„¤ì¹˜
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install fastapi uvicorn python-multipart aiofiles
!pip install diffusers transformers accelerate
!pip install "numpy==1.24.4" "opencv-python==4.9.0.80"
!pip install pydantic-settings pillow omegaconf
!pip install git+https://github.com/facebookresearch/segment-anything.git
```

#### 3.3 Colab ì „ìš© í™˜ê²½ ì„¤ì •
```python
# .env íŒŒì¼ ìƒì„±
env_content = '''# Model Configuration (GPU optimized for Colab)
SAM_MODEL_TYPE=vit_b
SAM_CHECKPOINT_PATH=./pretrained_models/sam_vit_b_01ec64.pth
LAMA_CONFIG_PATH=./lama/configs/prediction/default.yaml
LAMA_CHECKPOINT_PATH=./pretrained_models/big-lama
SD_MODEL_NAME=stabilityai/stable-diffusion-2-inpainting

# API Configuration
ENVIRONMENT=development
API_HOST=0.0.0.0
API_PORT=8000
API_PREFIX=/api/v1
DEBUG=true
MAX_IMAGE_SIZE=1024
MAX_FILE_SIZE=10485760

# CORS
ALLOWED_ORIGINS=*

# Device (GPU)
DEVICE=cuda

# GPU Optimization
TORCH_NUM_THREADS=4

# Mock Service (disabled for GPU testing)
USE_MOCK_SERVICE=false
'''

with open('.env', 'w') as f:
    f.write(env_content)

print("âœ… Environment configuration created")
```

#### 3.4 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```python
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
!chmod +x scripts/download_models.sh
!./scripts/download_models.sh
```

#### 3.5 ì„œë²„ ì‹œì‘ ë° í…ŒìŠ¤íŠ¸
```python
# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹œì‘
import subprocess
import time
import requests
import threading
import sys

def start_server():
    subprocess.run([
        sys.executable, "-m", "uvicorn", 
        "api.main:app", 
        "--host", "0.0.0.0", 
        "--port", "8000"
    ])

# ì„œë²„ ì‹œì‘
server_thread = threading.Thread(target=start_server, daemon=True)
server_thread.start()

# ì„œë²„ ì‹œì‘ ëŒ€ê¸°
print("â³ Waiting for server to start...")
time.sleep(20)

# í—¬ìŠ¤ì²´í¬
try:
    response = requests.get('http://localhost:8000/api/v1/health', timeout=10)
    print(f"âœ… Health check: {response.status_code}")
    print(response.json())
except Exception as e:
    print(f"âŒ Health check failed: {e}")
```

### 4. API í…ŒìŠ¤íŠ¸
```python
# ê°„ë‹¨í•œ API í…ŒìŠ¤íŠ¸
import requests
import base64
import numpy as np
from PIL import Image
import io

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
def create_test_image():
    img = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    pil_img = Image.fromarray(img)
    buffer = io.BytesIO()
    pil_img.save(buffer, format='PNG')
    buffer.seek(0)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

# API í…ŒìŠ¤íŠ¸ ìš”ì²­
test_data = {
    "image_data": f"data:image/png;base64,{create_test_image()}",
    "point_coords": [[256, 256]],
    "point_labels": [1],
    "dilate_kernel_size": 15
}

start_time = time.time()
response = requests.post('http://localhost:8000/api/v1/remove', json=test_data)
end_time = time.time()

print(f"ğŸš€ API response time: {end_time - start_time:.2f} seconds")
print(f"ğŸ“Š Status: {response.status_code}")

if response.status_code == 200:
    print("âœ… API test successful!")
    result = response.json()
    print(f"ğŸ“ Result image size: {len(result.get('result_image', ''))} characters")
else:
    print(f"âŒ API test failed: {response.text}")
```

### 5. ì™¸ë¶€ ì ‘ê·¼ ì„¤ì • (ì„ íƒì‚¬í•­)
```python
# ngrok ì„¤ì¹˜ ë° ì„¤ì •
!pip install pyngrok
from pyngrok import ngrok

# í„°ë„ ìƒì„± (ngrok ê³„ì • í•„ìš”)
public_url = ngrok.connect(8000)
print(f"ğŸŒ Public URL: {public_url}")
print(f"ğŸ“– API Documentation: {public_url}/docs")
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### GPU vs CPU ì„±ëŠ¥ ë¹„êµ
```python
import time

# SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ëŠ¥ ì¸¡ì •
def benchmark_sam():
    start_time = time.time()
    # SAM ì²˜ë¦¬ ì½”ë“œ (ìœ„ì˜ API í…ŒìŠ¤íŠ¸ ì¬ì‚¬ìš©)
    response = requests.post('http://localhost:8000/api/v1/remove', json=test_data)
    end_time = time.time()
    
    return end_time - start_time, response.status_code

# 5íšŒ í…ŒìŠ¤íŠ¸ í‰ê· 
times = []
for i in range(5):
    elapsed, status = benchmark_sam()
    if status == 200:
        times.append(elapsed)
    print(f"Test {i+1}: {elapsed:.2f}s (Status: {status})")

if times:
    avg_time = sum(times) / len(times)
    print(f"\nğŸ“Š Average response time: {avg_time:.2f}s")
    print(f"ğŸ”¥ GPU acceleration working!")
else:
    print("âŒ All tests failed")
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬
```python
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
torch.cuda.empty_cache()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
!nvidia-smi
```

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```python
# ëª¨ë¸ íŒŒì¼ í™•ì¸
!ls -la pretrained_models/
!du -sh pretrained_models/*
```

### ì„œë²„ ì‹œì‘ ì‹¤íŒ¨
```python
# ë¡œê·¸ í™•ì¸
!tail -20 server.log

# í¬íŠ¸ ì‚¬ìš© í™•ì¸
!netstat -tlnp | grep :8000
```

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì„¤ì • ì™„ë£Œ í™•ì¸
- [ ] GPU í™œì„±í™” í™•ì¸ (`nvidia-smi` ì„±ê³µ)
- [ ] í”„ë¡œì íŠ¸ í´ë¡  ì™„ë£Œ
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ (íŠ¹íˆ NumPy 1.24.4)
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (SAM, LaMa)
- [ ] í™˜ê²½ ì„¤ì • íŒŒì¼ (.env) ìƒì„±
- [ ] ì„œë²„ ì‹œì‘ ì„±ê³µ
- [ ] í—¬ìŠ¤ì²´í¬ API ì‘ë‹µ í™•ì¸

### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] `/api/v1/health` ì—”ë“œí¬ì¸íŠ¸
- [ ] `/api/v1/remove` ê°ì²´ ì œê±° API
- [ ] `/api/v1/fill` ì˜ì—­ ì±„ìš°ê¸° API
- [ ] `/api/v1/replace` ê°ì²´ êµì²´ API

### ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ
- [ ] API ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] CPU vs GPU ì„±ëŠ¥ ë¹„êµ

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### GPU í™˜ê²½ì—ì„œ ê¸°ëŒ€ë˜ëŠ” ì„±ëŠ¥:
- **SAM ì„¸ê·¸ë©˜í…Œì´ì…˜**: 2-5ì´ˆ
- **LaMa ì¸í˜ì¸íŒ…**: 3-8ì´ˆ
- **Stable Diffusion**: 10-30ì´ˆ

### CPU ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ:
- **SAM**: 5-10ë°° ë¹ ë¦„
- **ì „ì²´ íŒŒì´í”„ë¼ì¸**: 10-20ë°° ë¹ ë¦„

## ğŸ“ ë¬¸ì œ ë°œìƒ ì‹œ

1. **ìë™ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©**: `colab_setup.py` ì‹¤í–‰
2. **ë‹¨ê³„ë³„ ìˆ˜ë™ ì„¤ì •**: ìœ„ì˜ 3ë²ˆ í•­ëª© ì°¸ì¡°
3. **ë¡œê·¸ í™•ì¸**: ì„œë²„ ë¡œê·¸ì™€ ì—ëŸ¬ ë©”ì‹œì§€ ë¶„ì„
4. **ë©”ëª¨ë¦¬ ì •ë¦¬**: `torch.cuda.empty_cache()` ì‹¤í–‰

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼í•˜ë©´ Google Colabì—ì„œ ì™„ì „í•œ GPU ê°€ì† Inpaint Anything APIë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!